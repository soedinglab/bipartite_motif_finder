{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning bi-partite motifs based on a thermodynamic approach\n",
    "### Implements the dynamic programming and the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.special import logsumexp\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import multiprocessing\n",
    "import multiprocessing as mp\n",
    "import ctypes\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set compiler directives (cf. http://docs.cython.org/src/reference/compilation.html)\n",
    "from Cython.Compiler.Options import get_directive_defaults\n",
    "directive_defaults = get_directive_defaults()\n",
    "directive_defaults['linetrace'] = True\n",
    "directive_defaults['binding'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:61:91: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:62:122: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:64:86: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:67:86: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:70:132: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:72:138: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:74:135: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:77:91: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:79:91: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:81:91: cdef variable 'l' declared after it is used\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:120:37: Not all members given for struct 'DerParams'\n",
      "warning: /home/salma/.cache/ipython/cython/_cython_magic_729edb41b714b7cee9d525ebcb843fe5.pyx:120:37: Not all members given for struct 'DerParams'\n"
     ]
    }
   ],
   "source": [
    "%%cython -f -I . --compile-args=-DCYTHON_TRACE=1  --compile-args=-DAVX2=1  --compile-args=-mavx2 \n",
    "\n",
    "\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "ctypedef double c_float_t\n",
    "import numpy as np\n",
    "import itertools\n",
    "from libc.math cimport exp,pow\n",
    "\n",
    "float_dtype = np.NPY_DOUBLE\n",
    "np.import_array()\n",
    "\n",
    "cdef double cpi = np.pi\n",
    "cdef int l = 3 #l_A=l_B=3 nucleotides\n",
    "\n",
    "cpdef generate_kmer_inx():\n",
    "    cdef dict vals = {'A':0,'C':1,'G':2,'T':3}\n",
    "    cdef dict kmer_inx = {}\n",
    "    \n",
    "    for p in list(itertools.product(vals.keys(), repeat=l)):\n",
    "        inx = 0\n",
    "        for j,base in enumerate(p):\n",
    "            inx += (4**j)*vals[base] \n",
    "        kmer_inx[''.join(p)] = inx\n",
    "    return kmer_inx\n",
    "\n",
    "kmer_inx = generate_kmer_inx()\n",
    "inx_kmer = {y:x for x,y in kmer_inx.items()}\n",
    "\n",
    "cpdef seq2int_cy(str sequence):\n",
    "    cdef int L = len(sequence)\n",
    "    kmer_array = np.zeros(L, dtype=int)\n",
    "    \n",
    "    cdef i\n",
    "    for i in range(l-1,L):\n",
    "        kmer = sequence[i-l+1:i+1]\n",
    "        kmer_array[i] = kmer_inx[kmer]\n",
    "    return kmer_array        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "cdef extern from \"src_helper_avx.c\":\n",
    "    pass\n",
    "    \n",
    "cdef extern from \"src_helper_avx.h\":\n",
    "    ctypedef struct DerParams:\n",
    "        c_float_t* za_Ea_derivatives\n",
    "        c_float_t* zb_Ea_derivatives\n",
    "        c_float_t* za_Eb_derivatives\n",
    "        c_float_t* zb_Eb_derivatives\n",
    "        c_float_t* der_a\n",
    "        c_float_t* der_b\n",
    "        \n",
    "    void initialize_DerParams(DerParams* params, int L, int no_kmers)\n",
    "    void deinitialize_DerParams(DerParams* params)\n",
    "\n",
    "    double sum_array_c(double* arr, int length)\n",
    "    void sum_mat_rows(double* out, double* mat, int n_row, int n_col)\n",
    "    \n",
    "    void assign_za_c(int i, double* za, double* zb, double concentration_times_energy, int l)\n",
    "    void assign_zb_c(long* x, int i, double* za, double* zb, double* Eb, double cab, double sf, double D, double sig, int l)\n",
    "    \n",
    "    void assign_za_E_derivatives_c(long* x, int i, double* za, double* zb, int L, int l, int no_kmers,\n",
    "                                     DerParams* params, double* Ea, double* Eb, double cab)\n",
    "\n",
    "    void assign_zb_E_derivatives_c(long* x, int i, double* za, double* zb, int L, int l, int no_kmers,\n",
    "                                     DerParams* params, double* Ea, double* Eb, double cab, double sf, double D, double sig)\n",
    "\n",
    "    void assign_za_D_derivative_c(int i, double* za_D_derivatives, double* zb_D_derivatives, double concentration_times_energy, int l)\n",
    "\n",
    "    void assign_za_sig_derivative_c(int i, double* za_sig_derivatives, double* zb_sig_derivatives, double concentration_times_energy, int l)\n",
    "\n",
    "    void assign_za_sf_derivative_c(int i, double* za_sf_derivatives, double* zb_sf_derivatives, double concentration_times_energy, int l)\n",
    "    \n",
    "    void assign_zb_D_derivative_c(int i, double* za, double* za_D_derivatives, double* zb_D_derivatives, double energy_b, \n",
    "                                         double cab, double sf, double D , double sig, int l)\n",
    "    void assign_zb_sig_derivative_c(int i, double* za, double* za_sig_derivatives, double* zb_sig_derivatives, double energy_b, \n",
    "                                         double cab, double sf, double D , double sig, int l)\n",
    "    void assign_zb_sf_derivative_c(int i, double* za, double* za_sf_derivatives, double* zb_sf_derivatives, double energy_b, \n",
    "                                         double cab, double sf, double D , double sig, int l)\n",
    "\n",
    "    double cb_c(int, double, double, double)\n",
    "    double cb_D_derivative_c(int, double, double, double)\n",
    "    double cb_sig_derivative_c(int, double, double, double)\n",
    "    double cb_sf_derivative_c(int, double, double, double)\n",
    "    \n",
    "\n",
    "cdef array_from_pointer(void* ptr, int size):\n",
    "        cdef np.npy_intp shape_c[1]\n",
    "        shape_c[0] = <np.npy_intp> size\n",
    "        ndarray = np.PyArray_SimpleNewFromData(1, shape_c, float_dtype, ptr)\n",
    "        return ndarray\n",
    "    \n",
    "    \n",
    "@cython.boundscheck(False)  # Deactivate bounds checking   \n",
    "def DP_Z_cy(double[:] args, long[:] x):\n",
    "    \n",
    "    cdef int L = len(x)\n",
    "    cdef int no_kmers = len(kmer_inx)\n",
    "    cdef double cab = 1.0\n",
    "\n",
    "    cdef double[:] Ea = args[0:len(kmer_inx)]\n",
    "    cdef double[:] Eb = args[len(kmer_inx):2*len(kmer_inx)]\n",
    "    cdef double sf = args[-3]\n",
    "    cdef double D = args[-2]\n",
    "    cdef double sig = args[-1]\n",
    "    \n",
    "    #initialization of statistical weigths\n",
    "    cdef double[:] za = np.zeros(L)\n",
    "    cdef double[:] zb = np.zeros(L)\n",
    "\n",
    "    cdef int i\n",
    "    for i in range(0,l):\n",
    "        zb[i] = 1 \n",
    "        \n",
    "        \n",
    "\n",
    "    #initialization of derivatives\n",
    "    cdef DerParams params = DerParams()\n",
    "    initialize_DerParams(&params, L, no_kmers)\n",
    "\n",
    "\n",
    "    cdef double[:] za_sf_derivatives = np.zeros(L)\n",
    "    cdef double[:] zb_sf_derivatives = np.zeros(L)\n",
    "    \n",
    "    cdef double[:] za_D_derivatives = np.zeros(L)\n",
    "    cdef double[:] zb_D_derivatives = np.zeros(L)\n",
    "\n",
    "    cdef double[:] za_sig_derivatives = np.zeros(L)\n",
    "    cdef double[:] zb_sig_derivatives = np.zeros(L)\n",
    "\n",
    "\n",
    "    cdef int inx\n",
    "    \n",
    "    #precompute (binding energy of domain a binding at position i)*concentration\n",
    "    cdef double energy_conc_a \n",
    "    \n",
    "    #precompute binding energy of domain b binding at position i\n",
    "    cdef double energy_b\n",
    "    \n",
    "    #dynamic programming calculation of z and derivatives \n",
    "    \n",
    "    for i in range(l,L):\n",
    "        energy_conc_a = cab*exp(-Ea[x[i]])\n",
    "        energy_b = exp(-Eb[x[i]])\n",
    "        \n",
    "        #calculate statistical weights\n",
    "        assign_za_c(i, &za[0], &zb[0], energy_conc_a, l)\n",
    "        assign_zb_c(&x[0], i, &za[0], &zb[0], &Eb[0], cab, sf, D, sig, l)\n",
    "        \n",
    "        #calculate derivatives for all kmers (inx) at each position\n",
    "        assign_za_E_derivatives_c(&x[0], i, &za[0], &zb[0], L, l, len(kmer_inx), \n",
    "                                      &params, &Ea[0], &Eb[0], cab)\n",
    "        assign_zb_E_derivatives_c(&x[0], i, &za[0], &zb[0], L, l, len(kmer_inx), \n",
    "                                      &params, &Ea[0], &Eb[0], cab, sf, D, sig)\n",
    "        \n",
    "        \n",
    "        assign_za_sf_derivative_c(i, &za_sf_derivatives[0], &zb_sf_derivatives[0], energy_conc_a, l)\n",
    "        assign_zb_sf_derivative_c(i, &za[0], &za_sf_derivatives[0], &zb_sf_derivatives[0], energy_b, cab, sf, D, sig, l)\n",
    "        \n",
    "        assign_za_D_derivative_c(i, &za_D_derivatives[0], &zb_D_derivatives[0], energy_conc_a, l)\n",
    "        assign_zb_D_derivative_c(i, &za[0], &za_D_derivatives[0], &zb_D_derivatives[0], energy_b, cab, sf, D, sig, l)\n",
    "        \n",
    "        assign_za_sig_derivative_c(i, &za_sig_derivatives[0], &zb_sig_derivatives[0], energy_conc_a, l)\n",
    "        assign_zb_sig_derivative_c(i, &za[0], &za_sig_derivatives[0], &zb_sig_derivatives[0], energy_b, cab, sf, D, sig, l)\n",
    "\n",
    "    Z_x = zb[L-1] + sum_array_c(&za[0], L)\n",
    "    \n",
    "    #derivative of Z(x)\n",
    "    \n",
    "    za_Ea_derivatives = array_from_pointer(params.za_Ea_derivatives, no_kmers * L).reshape(L,-1)\n",
    "    za_Eb_derivatives = array_from_pointer(params.za_Eb_derivatives, no_kmers * L).reshape(L,-1)\n",
    "    zb_Ea_derivatives = array_from_pointer(params.zb_Ea_derivatives, no_kmers * L).reshape(L,-1)\n",
    "    zb_Eb_derivatives = array_from_pointer(params.zb_Eb_derivatives, no_kmers * L).reshape(L,-1)\n",
    "\n",
    "    d_Ea = zb_Ea_derivatives[L-1,:] + np.sum(za_Ea_derivatives, axis=0)\n",
    "    d_Eb = zb_Eb_derivatives[L-1,:] + np.sum(za_Eb_derivatives, axis=0)\n",
    "  \n",
    "    d_sf = zb_sf_derivatives[L-1] + sum_array_c(&za_sf_derivatives[0], L)\n",
    "    d_D = zb_D_derivatives[L-1] + sum_array_c(&za_D_derivatives[0], L)\n",
    "    d_sig = zb_sig_derivatives[L-1] + sum_array_c(&za_sig_derivatives[0], L)       \n",
    "    gradient = np.concatenate([q.ravel() for q in [d_Ea, d_Eb, np.array([d_sf, d_D, d_sig])]])\n",
    "\n",
    "    deinitialize_DerParams(&params)\n",
    "    \n",
    "    return Z_x, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation of the LL object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nLL:\n",
    "    def __init__(self, seqs_p, seqs_bg):\n",
    "        \n",
    "        self.N_p = len(seqs_p)\n",
    "        self.N_bg = len(seqs_bg)\n",
    "\n",
    "        #calculate background probabilities:\n",
    "\n",
    "        #include positive sequences in bg sequences if not there\n",
    "        X_bg_t = list(set(seqs_p + seqs_bg))  #number of unique sequences\n",
    "        \n",
    "        counts = np.zeros(len(X_bg_t))\n",
    "        for i, x in enumerate(X_bg_t):\n",
    "            counts[i] = seqs_bg.count(x)\n",
    "            \n",
    "        counts = counts + 1 #pseudocount to make sure \n",
    "        counts = counts/np.sum(counts)\n",
    "\n",
    "        p_bg = dict(zip(X_bg_t, counts))\n",
    "\n",
    "        self.pbg_xp = np.array([p_bg[x] for x in seqs_p])\n",
    "        self.pbg_xbg = np.array([p_bg[xbg] for xbg in seqs_bg])\n",
    "        \n",
    "        #add a padding nucleotide to the beginning to make the calculations stable, binding starts at\n",
    "        #position i=l so the padded nucleotide has no effect.\n",
    "        self.X_p = [seq2int_cy('A' + x) for x in seqs_p] \n",
    "        self.X_bg = [seq2int_cy('A' + x) for x in seqs_bg]\n",
    "\n",
    "        \n",
    "    def assign_z_p(self, tup):\n",
    "            i, args = tup\n",
    "            d_z_x_np = np.frombuffer(dz.get_obj(), dtype=np.float64).reshape(-1, self.N_p)\n",
    "            z[i], d_z_x_np[:,i] = DP_Z_cy(args, self.X_p[i])\n",
    "            \n",
    "    def assign_z_bg(self, tup):\n",
    "            i, args = tup\n",
    "            d_z_xbg_np = np.frombuffer(dz.get_obj(), dtype=np.float64).reshape(-1, self.N_bg)\n",
    "            z[i], d_z_xbg_np[:,i] = DP_Z_cy(args, self.X_bg[i])\n",
    "\n",
    "          \n",
    "\n",
    "        \n",
    "    def __call__(self, parameters):\n",
    "        \n",
    "        #number of positive variables (stacked at the end)\n",
    "        n_pos = 3\n",
    "        \n",
    "        #exp parameters to make sure they are positive\n",
    "        args = parameters.copy()\n",
    "        args[-n_pos:] = np.exp(args[-n_pos:])\n",
    "    \n",
    "    \n",
    "        #define weights and derivatives as a multiprocessing array\n",
    "        z_x = mp.Array(ctypes.c_double, self.N_p)\n",
    "        d_z_x = mp.Array(ctypes.c_double, (2*len(kmer_inx)+ n_pos)*self.N_p)\n",
    "\n",
    "        z_xbg = mp.Array(ctypes.c_double, self.N_bg)\n",
    "        d_z_xbg = mp.Array(ctypes.c_double, (2*len(kmer_inx)+ n_pos)*self.N_bg) \n",
    "        \n",
    "        #parallelizing\n",
    "        with multiprocessing.Pool(initializer=init, initargs=(z_x,d_z_x), processes=8) as pool:\n",
    "            pool.map(self.assign_z_p, [(i, args) for i in range(len(self.X_p))])\n",
    "        with multiprocessing.Pool(initializer=init, initargs=(z_xbg, d_z_xbg), processes=8)  as pool:\n",
    "            pool.map(self.assign_z_bg, [(i, args) for i in range(len(self.X_bg))])\n",
    "        \n",
    "        #= convert to np array ======\n",
    "        d_z_x = np.frombuffer(d_z_x.get_obj(), dtype=np.float64).reshape(-1, self.N_p)\n",
    "        d_z_xbg = np.frombuffer(d_z_xbg.get_obj(), dtype=np.float64).reshape(-1, self.N_bg)\n",
    "        z_x = np.frombuffer(z_x.get_obj(), dtype=np.float64)\n",
    "        z_xbg = np.frombuffer(z_xbg.get_obj(), dtype=np.float64)\n",
    "        #============================\n",
    "        \n",
    "        #calculate log likelihood of model given arg parameters\n",
    "        ll = np.sum(np.log(self.pbg_xp) + np.log(np.ones(self.N_p) - (np.ones(self.N_p)/z_x)))\n",
    "        ll -= self.N_p * logsumexp( np.log(self.pbg_xbg) + np.log(np.ones(self.N_bg) - (np.ones(self.N_bg)/z_xbg)) )\n",
    "        \n",
    "        #calculate partial derivatives of model given arg parameters\n",
    "        dll = np.sum(d_z_x/(z_x*(z_x-1)), axis=1)\n",
    "        dll -= self.N_p * ( np.sum((self.pbg_xbg * d_z_xbg)/(z_xbg*z_xbg), axis=1 ) / np.sum(self.pbg_xbg*(np.ones(self.N_bg) - (np.ones(self.N_bg)/z_xbg))))\n",
    "\n",
    "        #exp modify dLL for positive elements\n",
    "        dll[-n_pos:] = dll[-n_pos:]*args[-n_pos:]\n",
    "\n",
    "        #regularize some parameters\n",
    "        if False:\n",
    "            comp = -3\n",
    "            reg = 1e-8 \n",
    "            ll -= np.power(args[comp],2)*reg\n",
    "            dll[comp] -= 2*reg*args[comp]\n",
    "        return -ll, -dll \n",
    "\n",
    "#make the arrays global to all processes\n",
    "def init(z_array, dz_array):\n",
    "    global z\n",
    "    z = z_array    \n",
    "    global dz\n",
    "    dz = dz_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_evaluate(param, plus, bg):\n",
    "    z_plus = np.zeros(len(plus))\n",
    "    z_bg = np.zeros(len(bg))\n",
    "    \n",
    "    seq_pos = [seq2int_cy('A' + x) for x in plus]\n",
    "    seq_bg = [seq2int_cy('A' + x) for x in bg]\n",
    "    \n",
    "    n_pos = 3\n",
    "    #exp parameters to make sure they are positive\n",
    "    args = param.copy()\n",
    "    args[-n_pos:] = np.exp(args[-n_pos:])\n",
    "        \n",
    "    for i, x in enumerate(seq_pos):\n",
    "        z_plus[i], _ = DP_Z_cy(args, x)\n",
    "    \n",
    "    for i, x in enumerate(seq_bg):\n",
    "        z_bg[i], _ = DP_Z_cy(args, x)\n",
    "        \n",
    "    y_true = np.append(np.ones(len(plus)), np.zeros(len(bg)))\n",
    "    y_score = np.append(z_plus, z_bg)\n",
    "    \n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition (list_in, n):\n",
    "    random.shuffle(list_in)\n",
    "    return [list_in[i::n] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_performance(auc_validation, auc_train, param_history, theta):\n",
    "        \n",
    "    x = np.arange(1, len(auc_validation)+1, 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,3.5))\n",
    "    ax1.set_xlabel('10th iteration')\n",
    "    ax1.set_ylabel('AUC')\n",
    "    \n",
    "    x_max = max(30, len(auc_validation))\n",
    "    \n",
    "    ax1.set_xlim(0,x_max)\n",
    "    ax1.set_ylim(0.4,1)\n",
    "    \n",
    "    # get current motif ===========\n",
    "    core1 = {}\n",
    "    for i in range(len(kmer_inx)):\n",
    "        core1[inx_kmer[i]] = theta[i]\n",
    "\n",
    "    core1 = pd.Series(core1).sort_values(ascending=True)\n",
    "    \n",
    "    core2 = {}\n",
    "    for i in range(len(kmer_inx)):\n",
    "        core2[inx_kmer[i]] = theta[i+64]\n",
    "    core2 = pd.Series(core2).sort_values(ascending=True)\n",
    "    \n",
    "    d = np.exp(theta[-2])\n",
    "    sig = np.exp(theta[-1])\n",
    "    #===============================================\n",
    "\n",
    "    ax1.set_title('%s(%.2f) -- %.1f(%.1f) -- %s(%.2f)\\n%s(%.2f) ----------------- %s(%.2f)\\n%s(%.2f) ----------------- %s(%.2f)\\ncurrent validation AUC: %.3f\\nvariation index: %.2f'%(\n",
    "        core1.index[0], core1.values[0],d,sig,core2.index[0], core2.values[0], \n",
    "        core1.index[1], core1.values[1], core2.index[1], core2.values[1], \n",
    "        core1.index[2], core1.values[2], core2.index[2], core2.values[2],\n",
    "        auc_validation[-1],\n",
    "        param_local_fluctuation(param_history)),\n",
    "                loc='left')\n",
    "    ax1.plot(x, auc_validation, color='blue', label='validation set ')  \n",
    "    ax1.plot(x, auc_train, color='red', label='training set')\n",
    "    ax1.legend()\n",
    "    \n",
    "    #Plot binding energies of one kmer per core ====\n",
    "    \n",
    "    core1_hist = [arr[kmer_inx[kmer1]] for arr in param_history]\n",
    "    core2_hist = [arr[kmer_inx[kmer2]+ len(kmer_inx)] for arr in param_history]\n",
    "    \n",
    "    ax2.plot(x, core1_hist, color='blue', label='core1 %s E'%kmer1)\n",
    "    ax2.plot(x, core2_hist, color='red', label='core2 %s E'%kmer2)\n",
    "    ax3.set_xlabel('10th iteration')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(0,x_max)\n",
    "        \n",
    "    # plot sigma, D, and SF ========================\n",
    "    sf_hist = [np.exp(arr[-3]) for arr in param_history]\n",
    "    D_hist = [np.exp(arr[-2]) for arr in param_history]\n",
    "    sig_hist = [np.exp(arr[-1]) for arr in param_history]\n",
    "    \n",
    "    ax3.set_xlabel('10th iteration')\n",
    "    ax3.plot(x, D_hist, color='blue', label='D')  \n",
    "    ax3.plot(x, sig_hist, color='red', label='sigma')\n",
    "    \n",
    "    ax4 = ax3.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax4.set_ylabel('sf', color='green')  # we already handled the x-label with ax1\n",
    "    ax4.plot(x, sf_hist, color='green', label='sf')\n",
    "    ax4.tick_params(axis='y', labelcolor='green')    \n",
    "    \n",
    "    ax3.legend()    \n",
    "    ax3.set_xlim(0,x_max)\n",
    "    \n",
    "    #================================================\n",
    "    \n",
    "    plt.savefig('plots/'+ plot_name +'.pdf', bbox_inches='tight')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a fluctuation score [0-1] max representing parameter variation in the last 5 iterations.\n",
    "\n",
    "def param_local_fluctuation(param_history):\n",
    "    \n",
    "    last_params = list(param_history[-1])\n",
    "    min_energy_inx = last_params.index(min(last_params[:-3]))\n",
    "    energy_hist = [np.exp(arr[min_energy_inx]) for arr in param_history]\n",
    "    \n",
    "    D_hist = [np.exp(arr[-2]) for arr in param_history]\n",
    "    sig_hist = [np.exp(arr[-1]) for arr in param_history]\n",
    "    \n",
    "    #define the strech which is assigned as local\n",
    "    #if less than 5 iterations --> return 1\n",
    "    if len(param_history)<5:\n",
    "        return 1\n",
    "    else:\n",
    "        loc_len = 5\n",
    "       \n",
    "    #max(arr)-min(arr) for the last 5 elements of this parameter in adam optimization\n",
    "    local_variation=np.array([max(a)-min(a) for a in [energy_hist[-loc_len:], D_hist[-loc_len:], sig_hist[-loc_len:]]])\n",
    "    \n",
    "    #max(arr)-min(arr) for all parameter history in adam optimization\n",
    "    #global_variation=np.array([max(a)-min(a) for a in [energy_hist, D_hist, sig_hist]])\n",
    "        \n",
    "    #return biggest ratio of local to absolute value\n",
    "    return max(local_variation/(np.array([energy_hist[-1], D_hist[-1], sig_hist[-1]])+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000%5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_adam(plus, bg, plus_valid, bg_valid, red_thr=10, var_thr=0.05, \n",
    "                  sequences_per_batch=100, max_iterations=1000, evaluate_after=None):\n",
    "\n",
    "    #number of minibatches: number of positives/numbers per batch\n",
    "    n_batch = int(len(plus)/sequences_per_batch)\n",
    "    \n",
    "    #adam parameters\n",
    "    alpha = 0.01\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.999   \n",
    "    epsilon = 1e-8\n",
    "\n",
    "    #initialize the vector\n",
    "    theta_0 = parameters  \n",
    "    n_param = len(parameters)\n",
    "\n",
    "    m_t = np.zeros(n_param) \n",
    "    v_t = np.zeros(n_param)  \n",
    "\n",
    "    t = 0  #iterations\n",
    "    f_t = 42 #initialize with random number\n",
    "    epoch = 0\n",
    "\n",
    "\n",
    "    #auc array tracks auc values\n",
    "    auc_validation = []\n",
    "    auc_train = []\n",
    "    param_history = []    \n",
    "    \n",
    "    #calculate the stretch of reduced performance\n",
    "    reduction = 0\n",
    "    \n",
    "    #if none assign an epoch evaluation scheme\n",
    "    if evaluate_after is None:\n",
    "        evaluate_after = len(plus)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        #split data into several minibatches\n",
    "        epoch += 1\n",
    "\n",
    "        pos_batches = partition(plus, n_batch)\n",
    "        bg_batches  = partition(bg, n_batch)\n",
    "        \n",
    "        #enumerate minibatches\n",
    "        for i in range(n_batch):\n",
    "\n",
    "            nll_obj = nLL(pos_batches[i],bg_batches[i])\n",
    "\n",
    "            t+=1\n",
    "\n",
    "            f_prev = f_t\n",
    "\n",
    "            #computes the gradient of the stochastic function\n",
    "            f_t, g_t = nll_obj(theta_0) \n",
    "\n",
    "            #updates the moving averages of the gradient\n",
    "            m_t = beta_1*m_t + (1-beta_1)*g_t \n",
    "\n",
    "            #updates the moving averages of the squared gradient\n",
    "            v_t = beta_2*v_t + (1-beta_2)*(g_t*g_t) \n",
    "\n",
    "            #calculates the bias-corrected estimates\n",
    "            m_cap = m_t/(1-(beta_1**t)) \n",
    "\n",
    "            #calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(beta_2**t)) \n",
    "\n",
    "            #before updating parameters plot performance on validation set\n",
    "\n",
    "            if (t*sequences_per_batch)%evaluate_after < sequences_per_batch:  #every now and then\n",
    "                aucv = auc_evaluate(theta_0, plus_valid, bg_valid)\n",
    "                auct = auc_evaluate(theta_0, plus, bg)\n",
    "                auc_validation.append(aucv)\n",
    "                auc_train.append(auct)\n",
    "                param_history.append(theta_0)\n",
    "                plt_performance(auc_validation, auc_train, param_history, theta_0)\n",
    "                \n",
    "                if len(auc_validation)>5:\n",
    "                    if auc_validation[-1] <= auc_validation[-2]:\n",
    "                        reduction += 1\n",
    "\n",
    "                    #stop when validation set performs worse for at  least red_thr times (variation) and when the parameters are stable\n",
    "                    variability_index = param_local_fluctuation(param_history)\n",
    "                    if (reduction > red_thr) and (variability_index<var_thr):\n",
    "                        np.savetxt(fname='param/'+ plot_name +'.txt', X=np.insert(theta_0,0,f_t))\n",
    "                        return theta_0, g_t\n",
    "\n",
    "                    if t>max_iterations:\n",
    "                        np.savetxt(fname='param/'+ plot_name +'.txt', X=np.insert(theta_0,0,f_t))\n",
    "                        return theta_0, g_t\n",
    "                \n",
    "                \n",
    "            #updates the parameters by moving a step towards gradients\n",
    "            theta_0_prev = theta_0 \n",
    "            theta_0 = theta_0 - (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import fasta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_fasta(file_name):\n",
    "    input_seq_iterator = SeqIO.parse(file_name, \"fasta\")\n",
    "    return [str(record.seq) for record in input_seq_iterator]\n",
    "\n",
    "def parse_fastq(file_name):\n",
    "    input_seq_iterator = SeqIO.parse(file_name, \"fastq\")\n",
    "    return [str(record.seq) for record in input_seq_iterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def swap_cores (args):\n",
    "    core1_args = args[:len(kmer_inx)]\n",
    "    core2_args = args[len(kmer_inx):len(kmer_inx)*2]\n",
    "    return np.concatenate([x.ravel() for x in [core2_args, core1_args, np.array([args[-2], args[-1]])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    set_size = 15000\n",
    "    bg = parse_fasta('HNRNPA0_1_TGTCGA40NCCGA_AAG_1.fasta.tmp')\n",
    "    plus = parse_fasta('HNRNPA0_4_TGTCGA40NCCGA_AAG_4.fasta.tmp')\n",
    "\n",
    "    bg = random.sample(bg, set_size)\n",
    "    plus = random.sample(plus, set_size)\n",
    "    \n",
    "    bg   = [seq.replace('N', random.sample(['A','T','C','G'],1)[0]) for seq in bg]\n",
    "    plus = [seq.replace('N', random.sample(['A','T','C','G'],1)[0]) for seq in plus]\n",
    "    \n",
    "else:\n",
    "    background_set = parse_fasta('negatives_toy.fasta')\n",
    "    positive_set = parse_fasta('positives_toy.fasta')\n",
    "    \n",
    "    bg_train, bg_test, bg_valid = partition(background_set, 3)\n",
    "    pos_train, pos_test, pos_valid = partition(positive_set, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer1 = 'TGT'\n",
    "kmer2 = 'GTG'\n",
    "%matplotlib inline\n",
    "    \n",
    "for i in range(1,2):\n",
    "    Ea = np.random.normal(loc=12.0, scale=1.0, size=len(kmer_inx))\n",
    "    Eb = np.random.normal(loc=12.0, scale=1.0, size=len(kmer_inx))\n",
    "    sf = np.log(10000)\n",
    "    D = np.log(np.random.uniform(1,15))\n",
    "    sig = np.log(np.random.uniform(1,15))\n",
    "\n",
    "    parameters = np.concatenate([x.ravel() for x in [Ea, Eb, np.array([sf, D, sig])]])\n",
    "\n",
    "    identifier = random.randint(1,999)\n",
    "    n = 500\n",
    "    red_thr = 20\n",
    "    var_thr = 0.03\n",
    "    seq_per_batch = 50\n",
    "    plot_name = 'TAG_TAG_%dseqperbatch_%d_%d_ADAM'%(seq_per_batch,n,identifier)\n",
    "    \n",
    "    maxiter=1000\n",
    "    x_opt, grad = optimize_adam(random.sample(pos_train, n), random.sample(bg_train, n),  \n",
    "                                random.sample(pos_valid, n), random.sample(bg_valid, n), \n",
    "                                red_thr, var_thr, sequences_per_batch=seq_per_batch, max_iterations=maxiter, evaluate_after=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rbp_motif]",
   "language": "python",
   "name": "conda-env-rbp_motif-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
